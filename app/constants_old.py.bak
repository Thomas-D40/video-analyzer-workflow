"""
Application Constants and Enums

This module centralizes all constants and enumerations used throughout the application
to maintain consistency and enable type-safe code.
"""

from enum import Enum


class AnalysisMode(str, Enum):
    """
    Analysis modes determining research depth and source types.

    - SIMPLE: Fast analysis using only abstracts
    - MEDIUM: Balanced analysis with 3 full-text papers
    - HARD: In-depth analysis with 6 full-text papers
    """
    SIMPLE = "simple"
    MEDIUM = "medium"
    HARD = "hard"

    @property
    def description(self) -> str:
        """Returns human-readable description in French."""
        descriptions = {
            self.SIMPLE: "Rapide (abstracts uniquement)",
            self.MEDIUM: "Équilibré (3 textes complets)",
            self.HARD: "Approfondi (6 textes complets)"
        }
        return descriptions[self]

    @property
    def full_text_count(self) -> int:
        """Returns the number of full-text papers to fetch."""
        counts = {
            self.SIMPLE: 0,
            self.MEDIUM: 3,
            self.HARD: 6
        }
        return counts[self]


class AnalysisStatus(str, Enum):
    """
    Status of a video analysis.

    - PENDING: Analysis has been requested but not yet started
    - COMPLETED: Analysis has finished successfully
    - FAILED: Analysis encountered an error and did not complete
    """
    PENDING = "pending"
    COMPLETED = "completed"
    FAILED = "failed"


class CacheReason(str, Enum):
    """
    Reasons for cache hit/miss decisions.

    Used in cache metadata to explain why a cached result was used or not.
    """
    EXACT_MATCH = "exact_match"
    UPGRADED_MODE = "upgraded_mode"
    NO_CACHE = "no_cache"
    TOO_OLD = "too_old"
    FORCE_REFRESH = "force_refresh"


# Cache configuration
CACHE_MAX_AGE_DAYS = 7
"""Maximum age (in days) for cached analyses to be considered fresh."""

# Transcript limits
TRANSCRIPT_MIN_LENGTH = 50
"""Minimum transcript length to proceed with analysis."""

TRANSCRIPT_MAX_LENGTH_FOR_ARGS = 25000
"""Maximum transcript characters to send to argument extraction (token optimization)."""

# Rating system
RATING_MIN = 0.0
"""Minimum user rating value."""

RATING_MAX = 5.0
"""Maximum user rating value."""

# OpenAI models
OPENAI_MODEL_FAST = "gpt-4o-mini"
"""Fast model for pros/cons analysis."""

OPENAI_MODEL_SMART = "gpt-4o"
"""Smart model for argument extraction."""

# Source types
class SourceType(str, Enum):
    """Types of research sources."""
    WEB = "web"
    SCIENTIFIC = "scientific"
    STATISTICAL = "statistical"


# Access types for research papers
class AccessType(str, Enum):
    """Access level for research papers."""
    OPEN = "open"
    CLOSED = "closed"
    UNKNOWN = "unknown"


# ============================================================================
# API & NETWORK CONFIGURATION
# ============================================================================

# Timeouts
DEFAULT_TIMEOUT_HTTPX = 60  # seconds
PUBMED_REQUEST_TIMEOUT = 10  # seconds
PUBMED_FETCH_TIMEOUT = 15  # seconds
SUBTITLE_DOWNLOAD_TIMEOUT_SECONDS = 10  # seconds
MCP_WEB_FETCH_DEFAULT_TIMEOUT = 30  # seconds
MCP_REQUEST_TIMEOUT = 30  # seconds

# Rate Limits (calls per second)
PUBMED_RATE_LIMIT_WITHOUT_KEY = 0.34  # ~3 requests per second
PUBMED_RATE_LIMIT_WITH_KEY = 0.11  # ~10 requests per second
RATE_LIMIT_OECD_CALLS_PER_SEC = 1.0
RATE_LIMIT_WORLD_BANK_CALLS_PER_SEC = 2.0
RATE_LIMIT_ARXIV_CALLS_PER_SEC = 1.0
RATE_LIMIT_PUBMED_CALLS_PER_SEC = 3.0
RATE_LIMIT_SEMANTIC_SCHOLAR_CALLS_PER_SEC = 1.0

# Circuit Breaker Configuration
DEFAULT_CIRCUIT_BREAKER_THRESHOLD = 5  # failures before opening circuit
CIRCUIT_BREAKER_RECOVERY_TIMEOUT_OECD = 300  # seconds (5 minutes)
CIRCUIT_BREAKER_RECOVERY_TIMEOUT_ACADEMIC = 180  # seconds (3 minutes)

# ============================================================================
# CONTENT & TOKEN LIMITS
# ============================================================================

# Transcript processing
TRANSCRIPT_MIN_VALID_LENGTH = 100  # minimum chars to validate transcript
TRANSCRIPT_MAX_LENGTH_FOR_ARGS = 25000  # max chars for argument extraction

# Analysis content limits
PROS_CONS_MAX_CONTENT_LENGTH = 40000  # max content for pros/cons analysis
PROS_CONS_MIN_PARTIAL_CONTENT = 500  # min space for partial content
AGGREGATE_MAX_PROS_PER_ARG = 5  # max pros items per argument
AGGREGATE_MAX_CONS_PER_ARG = 5  # max cons items per argument
AGGREGATE_MAX_CLAIM_LENGTH = 200  # max chars per claim
AGGREGATE_MAX_ARGUMENT_LENGTH = 300  # max chars for argument text
AGGREGATE_MAX_ITEMS_TEXT_LENGTH = 10000  # max length for items text

# Research result limits
PUBMED_SNIPPET_MAX_LENGTH = 500  # max snippet length
PUBMED_MAX_AUTHORS_DISPLAYED = 3  # max authors to display
FULLTEXT_MAX_CONTENT_LENGTH = 50000  # max full-text content
SCREENING_TITLE_MAX_LENGTH = 150  # max title for screening
SCREENING_SNIPPET_MAX_LENGTH = 300  # max snippet for screening
SCREENING_MAX_TOKENS = 800  # max tokens for LLM screening

# ============================================================================
# RESEARCH & ANALYSIS DEFAULTS
# ============================================================================

# Default max results per source
PUBMED_DEFAULT_MAX_RESULTS = 5
ARXIV_DEFAULT_MAX_RESULTS = 5
OECD_DEFAULT_MAX_RESULTS = 3
WORLD_BANK_DEFAULT_YEARS = 1
WORLD_BANK_DEFAULT_MAX_INDICATORS = 3
WORLD_BANK_MRV_YEARS = 5  # Most Recent Value years
WORLD_BANK_MAX_KEYWORDS = 5
WORLD_BANK_MAX_COUNTRIES = 5

# Parallel research configuration
PARALLEL_RESEARCH_MAX_WORKERS = 10  # thread pool size

# Analysis mode thresholds
ANALYSIS_MODE_MEDIUM_MIN_SCORE = 0.6  # min relevance for medium mode
ANALYSIS_MODE_HARD_MIN_SCORE = 0.5  # min relevance for hard mode

# ============================================================================
# RELIABILITY & SCORING
# ============================================================================

# Reliability calculation
RELIABILITY_NO_SOURCES = 0.0  # score when no sources available
RELIABILITY_MAX_FALLBACK = 0.9  # max score in fallback calculation
RELIABILITY_BASE_SCORE = 0.3  # base score for fallback
RELIABILITY_PER_SOURCE_INCREMENT = 0.1  # score increase per source

# Composite score weights (must sum to 1.0)
COMPOSITE_SCORE_QUALITY_WEIGHT = 0.40  # weight for analysis quality
COMPOSITE_SCORE_RATING_WEIGHT = 0.30  # weight for user rating
COMPOSITE_SCORE_CONFIDENCE_WEIGHT = 0.20  # weight for vote confidence
COMPOSITE_SCORE_RECENCY_WEIGHT = 0.10  # weight for freshness
COMPOSITE_SCORE_CONFIDENCE_DIVISOR = 2.0  # divisor for log scaling

# Relevance thresholds
DEFAULT_MIN_RELEVANCE_SCORE = 0.6  # default min relevance score
RELEVANCE_THRESHOLD_HIGH = 0.7  # high relevance threshold
RELEVANCE_THRESHOLD_MEDIUM_MIN = 0.4  # medium relevance min
RELEVANCE_THRESHOLD_MEDIUM_MAX = 0.7  # medium relevance max
RELEVANCE_THRESHOLD_LOW = 0.4  # low relevance threshold
RELEVANCE_DEFAULT_MIN_SCORE = 0.2  # default min for filtering
RELEVANCE_DEFAULT_MAX_RESULTS = 2  # default max results for filtering

# ============================================================================
# RETRY & BACKOFF CONFIGURATION
# ============================================================================

# Default retry settings
DEFAULT_RETRY_MAX_ATTEMPTS = 3
DEFAULT_RETRY_BASE_DELAY = 1.0  # seconds
DEFAULT_RETRY_BACKOFF_FACTOR = 2.0  # exponential multiplier
DEFAULT_RETRY_MAX_DELAY = 60.0  # seconds

# Service-specific retries
QUERY_GENERATOR_MAX_RETRY_ATTEMPTS = 2
QUERY_GENERATOR_BASE_DELAY = 1.0
WORLD_BANK_MAX_RETRY_ATTEMPTS = 3
WORLD_BANK_BASE_DELAY = 1.0
WORLD_BANK_FETCH_MAX_RETRY_ATTEMPTS = 2
WORLD_BANK_FETCH_BASE_DELAY = 1.0

# ============================================================================
# LLM TEMPERATURE SETTINGS
# ============================================================================

LLM_TEMP_ARGUMENT_EXTRACTION = 0.2  # for extracting arguments
LLM_TEMP_PROS_CONS_ANALYSIS = 0.3  # for pros/cons analysis
LLM_TEMP_RELIABILITY_AGGREGATION = 0.2  # for reliability scoring
LLM_TEMP_TOPIC_CLASSIFICATION = 0.3  # for topic classification
LLM_TEMP_QUERY_GENERATION = 0.3  # for query generation
LLM_TEMP_ARXIV_KEYWORDS = 0.3  # for ArXiv keyword extraction
LLM_TEMP_RELEVANCE_SCREENING = 0.1  # for relevance screening

# ============================================================================
# KEYWORD & TEXT PROCESSING
# ============================================================================

KEYWORD_MIN_LENGTH = 3  # minimum word length for keyword extraction
ARXIV_MIN_WORD_LENGTH_FALLBACK = 4
ARXIV_MAX_KEYWORDS_FALLBACK = 4
QUERY_GEN_MIN_WORD_LENGTH = 3
QUERY_GEN_MAX_KEYWORDS = 5

# ============================================================================
# YOUTUBE CONFIGURATION
# ============================================================================

YOUTUBE_VIDEO_ID_LENGTH = 11  # standard YouTube video ID length
TEMP_COOKIE_FILE_PREFIX = "/tmp/cookies_"  # prefix for cookie files

# ============================================================================
# OECD-SPECIFIC CONSTANTS
# ============================================================================

OECD_KEYWORD_MATCH_SCORE = 2  # score for exact keyword match
OECD_NAME_WORD_MATCH_SCORE = 1.5  # score for name word match
OECD_DESC_WORD_MATCH_SCORE = 1  # score for description word match
OECD_TOP_MATCH_LIMIT = 3  # number of top matches to return

# ============================================================================
# STOP WORDS (for keyword extraction)
# ============================================================================

FRENCH_STOP_WORDS = {
    "le", "la", "les", "un", "une", "des", "de", "du", "et", "ou",
    "mais", "donc", "car", "ni", "que", "qui", "quoi", "dont", "où",
    "ce", "cet", "cette", "ces", "mon", "ton", "son", "notre", "votre",
    "leur", "mes", "tes", "ses", "nos", "vos", "leurs", "je", "tu",
    "il", "elle", "on", "nous", "vous", "ils", "elles", "être", "avoir",
    "faire", "dire", "aller", "voir", "savoir", "pouvoir", "vouloir",
    "falloir", "devoir", "croire", "prendre", "donner", "tenir", "venir",
    "trouver", "mettre", "passer", "tout", "tous", "toute", "toutes",
    "pour", "dans", "par", "sur", "avec", "sans", "sous", "vers", "chez",
    "entre", "depuis", "pendant", "comme", "si", "plus", "moins", "très",
    "bien", "peu", "beaucoup", "trop", "assez", "encore", "déjà", "jamais",
    "toujours", "souvent", "parfois", "aussi", "ainsi", "alors", "après",
    "avant", "maintenant", "ici", "là", "partout", "ailleurs", "aujourd'hui",
    "hier", "demain", "ne", "pas", "point", "rien", "aucun", "personne",
    "quelque", "plusieurs", "autre", "même", "tel", "quel", "quelle", "quels"
}

COMMON_STOP_WORDS_EN_FR = {
    # English
    "the", "a", "an", "and", "or", "but", "if", "then", "else", "when",
    "at", "from", "by", "to", "of", "in", "on", "for", "with", "about",
    "as", "is", "are", "was", "were", "be", "been", "being", "have", "has",
    "had", "do", "does", "did", "will", "would", "should", "could", "may",
    "might", "must", "can", "this", "that", "these", "those", "i", "you",
    "he", "she", "it", "we", "they", "what", "which", "who", "when", "where",
    "why", "how", "all", "each", "every", "both", "few", "more", "most",
    "other", "some", "such", "no", "nor", "not", "only", "own", "same",
    "so", "than", "too", "very", "s", "t", "just", "now", "d", "ll", "m",
    "o", "re", "ve", "y", "ain", "aren", "couldn", "didn", "doesn", "hadn",
    "hasn", "haven", "isn", "ma", "mightn", "mustn", "needn", "shan",
    "shouldn", "wasn", "weren", "won", "wouldn"
} | FRENCH_STOP_WORDS  # Combine with French stop words
